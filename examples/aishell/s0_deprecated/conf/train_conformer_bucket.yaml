# network architecture
# encoder related
encoder: conformer
encoder_conf:
    output_size: 256    # dimension of attention
    attention_heads: 4
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 12      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: conv2d # encoder input type, you can chose conv2d, conv2d6 and conv2d8
    normalize_before: true
    cnn_module_kernel: 15
    use_cnn_module: True
    activation_type: 'swish'
    pos_enc_layer_type: 'rel_pos'
    selfattention_layer_type: 'rel_selfattn'

# decoder related
decoder: transformer
decoder_conf:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0

# hybrid CTC/attention
model_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1     # label smoothing option
    length_normalized_loss: false

# feature extraction
collate_conf:
    # waveform level config
    wav_distortion_conf:
        wav_dither: 0.1
        wav_distortion_rate: 0.0
        distortion_methods: []
    speed_perturb: true
    feature_extraction_conf:
        feature_type: 'fbank'
        mel_bins: 80
        frame_shift: 10
        frame_length: 25
        using_pitch: false
    # spec level config
    # spec_swap: false
    feature_dither: 0.0 # add dither [-feature_dither,feature_dither] on fbank feature
    spec_aug: true
    spec_aug_conf:
        warp_for_time: false
        num_t_mask: 2
        num_f_mask: 2
        max_t: 50
        max_f: 10
        max_w: 80


# dataset related
dataset_conf:
    max_length: 3000
    min_length: 0
    token_max_length: 30
    token_min_length: 1
    batch_type: 'bucket'    # bucket, static, dynamic
    frame_bucket_limit: '144, 204, 288, 400, 512, 600, 712, 800, 912, 1024, 1112, 1200, 1400, 1600, 2000, 3000'
    batch_bucket_limit: '100, 200, 200, 180, 180, 140, 140, 140, 100, 100, 100, 100, 60, 20, 20, 20'
    batch_factor: 0.4
    shuffle: True

grad_clip: 5
accum_grad: 1
max_epoch: 240
log_interval: 100

optim: adam
optim_conf:
    lr: 0.002
scheduler: warmuplr     # pytorch v1.1.0+ required
scheduler_conf:
    warmup_steps: 25000
